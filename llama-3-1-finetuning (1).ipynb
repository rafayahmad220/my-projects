{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30763,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -U transformers datasets accelerate peft trl bitsandbytes wandb -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-18T16:27:54.300676Z","iopub.execute_input":"2024-09-18T16:27:54.301214Z","iopub.status.idle":"2024-09-18T16:28:41.156137Z","shell.execute_reply.started":"2024-09-18T16:27:54.301177Z","shell.execute_reply":"2024-09-18T16:28:41.155050Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bitsandbytes -q","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:28:41.157947Z","iopub.execute_input":"2024-09-18T16:28:41.158269Z","iopub.status.idle":"2024-09-18T16:28:53.937202Z","shell.execute_reply.started":"2024-09-18T16:28:41.158229Z","shell.execute_reply":"2024-09-18T16:28:53.936032Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:28:53.938595Z","iopub.execute_input":"2024-09-18T16:28:53.938895Z","iopub.status.idle":"2024-09-18T16:28:58.809147Z","shell.execute_reply.started":"2024-09-18T16:28:53.938862Z","shell.execute_reply":"2024-09-18T16:28:58.808276Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import transformers\nimport datasets\nimport torch\nimport wandb\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n\nprint(\"Transformers version:\", transformers.__version__)\nprint(\"Datasets version:\", datasets.__version__)\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"WandB version:\", wandb.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:28:58.811190Z","iopub.execute_input":"2024-09-18T16:28:58.811631Z","iopub.status.idle":"2024-09-18T16:29:03.570142Z","shell.execute_reply.started":"2024-09-18T16:28:58.811599Z","shell.execute_reply":"2024-09-18T16:29:03.569230Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Transformers version: 4.44.2\nDatasets version: 3.0.0\nPyTorch version: 2.4.0\nWandB version: 0.18.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n#Store the Hugging Face token in environment\nos.environ[\"HF_TOKEN\"] = \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:03.571325Z","iopub.execute_input":"2024-09-18T16:29:03.571804Z","iopub.status.idle":"2024-09-18T16:29:03.576345Z","shell.execute_reply.started":"2024-09-18T16:29:03.571770Z","shell.execute_reply":"2024-09-18T16:29:03.575340Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=os.getenv(\"HF_TOKEN\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:03.577534Z","iopub.execute_input":"2024-09-18T16:29:03.577873Z","iopub.status.idle":"2024-09-18T16:29:05.473848Z","shell.execute_reply.started":"2024-09-18T16:29:03.577833Z","shell.execute_reply":"2024-09-18T16:29:05.472936Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"#Preparing workspace\nos.makedirs('models', exist_ok=True)\nos.makedirs('output', exist_ok=True)\nos.makedirs('logs', exist_ok=True)\nprint(\"Workspace directories are set up.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:05.475089Z","iopub.execute_input":"2024-09-18T16:29:05.477191Z","iopub.status.idle":"2024-09-18T16:29:05.483270Z","shell.execute_reply.started":"2024-09-18T16:29:05.477142Z","shell.execute_reply":"2024-09-18T16:29:05.482219Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Workspace directories are set up.\n","output_type":"stream"}]},{"cell_type":"code","source":"#Testing Configuration by loading dummy data point in W&B\n\n# Initialize a WandB run\nwandb.init(project='llama3-finetuning', entity='rafayahmad-addo-ai')\n\n# Log a test data point\nwandb.log({'test': 1})\n\n# Finish the run\nwandb.finish()\nprint(\"W&B setup test completed successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:05.484905Z","iopub.execute_input":"2024-09-18T16:29:05.485347Z","iopub.status.idle":"2024-09-18T16:29:09.585813Z","shell.execute_reply.started":"2024-09-18T16:29:05.485303Z","shell.execute_reply":"2024-09-18T16:29:09.584849Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrafayahmad\u001b[0m (\u001b[33mrafayahmad-addo-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240918_162905-w4dsin24</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning/runs/w4dsin24' target=\"_blank\">bright-pond-13</a></strong> to <a href='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning' target=\"_blank\">https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning/runs/w4dsin24' target=\"_blank\">https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning/runs/w4dsin24</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test</td><td>1</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bright-pond-13</strong> at: <a href='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning/runs/w4dsin24' target=\"_blank\">https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning/runs/w4dsin24</a><br/> View project at: <a href='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning' target=\"_blank\">https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240918_162905-w4dsin24/logs</code>"},"metadata":{}},{"name":"stdout","text":"W&B setup test completed successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"#Loading and splitting dataset \nfrom datasets import load_dataset\ndataset = load_dataset(\"keivalya/MedQuad-MedicalQnADataset\")\ntrain_test_split = dataset['train'].train_test_split(test_size=0.1, shuffle=True, seed = 42)\ntrain_dataset = train_test_split['train']\ntest_dataset = train_test_split['test']","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:09.587181Z","iopub.execute_input":"2024-09-18T16:29:09.587856Z","iopub.status.idle":"2024-09-18T16:29:12.830675Z","shell.execute_reply.started":"2024-09-18T16:29:09.587811Z","shell.execute_reply":"2024-09-18T16:29:12.829908Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"630b68e0cd0e421d980073a236a58851"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medDataset_processed.csv:   0%|          | 0.00/22.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aea3ef825ff4b8bb054193971bb3fb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/16407 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6939d42baa484e998226ff7d43e1b2c1"}},"metadata":{}}]},{"cell_type":"code","source":"#Inspect the dataset\nprint(\"Dataset structure:\", dataset)\nprint(\"Sample entry:\", dataset['train'][0])","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:12.833923Z","iopub.execute_input":"2024-09-18T16:29:12.834588Z","iopub.status.idle":"2024-09-18T16:29:12.841648Z","shell.execute_reply.started":"2024-09-18T16:29:12.834553Z","shell.execute_reply":"2024-09-18T16:29:12.840607Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Dataset structure: DatasetDict({\n    train: Dataset({\n        features: ['qtype', 'Question', 'Answer'],\n        num_rows: 16407\n    })\n})\nSample entry: {'qtype': 'susceptibility', 'Question': 'Who is at risk for Lymphocytic Choriomeningitis (LCM)? ?', 'Answer': 'LCMV infections can occur after exposure to fresh urine, droppings, saliva, or nesting materials from infected rodents.  Transmission may also occur when these materials are directly introduced into broken skin, the nose, the eyes, or the mouth, or presumably, via the bite of an infected rodent. Person-to-person transmission has not been reported, with the exception of vertical transmission from infected mother to fetus, and rarely, through organ transplantation.'}\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:12.842799Z","iopub.execute_input":"2024-09-18T16:29:12.843266Z","iopub.status.idle":"2024-09-18T16:29:14.736099Z","shell.execute_reply.started":"2024-09-18T16:29:12.843212Z","shell.execute_reply":"2024-09-18T16:29:14.735119Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98e7b1652860490f9f47fa1f5566cde1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cea17558f35f43dcb767b58ec10c23ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64783efaf7834ea1a34d270df611617b"}},"metadata":{}}]},{"cell_type":"code","source":"#Add a custom padding token\n# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntokenizer.pad_token_id = tokenizer.eos_token_id\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:14.737418Z","iopub.execute_input":"2024-09-18T16:29:14.737728Z","iopub.status.idle":"2024-09-18T16:29:14.742319Z","shell.execute_reply.started":"2024-09-18T16:29:14.737696Z","shell.execute_reply":"2024-09-18T16:29:14.741166Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Data Preprocessing (cleaning text, tokenizing, and converting entries to a format suitable for model training)\n\ndef preprocess_data(example):\n    # Tokenize the conversations\n    return tokenizer(example['qtype'], truncation=True, padding=\"max_length\", max_length=512)\n\n# Apply the preprocessing function to all entries in the dataset\ndataset = dataset.map(preprocess_data, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:14.743963Z","iopub.execute_input":"2024-09-18T16:29:14.744375Z","iopub.status.idle":"2024-09-18T16:29:19.095387Z","shell.execute_reply.started":"2024-09-18T16:29:14.744332Z","shell.execute_reply":"2024-09-18T16:29:19.094473Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16407 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4216f79daca348d0811cbe7c68a708d8"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Model and Tokenizer Configuration","metadata":{}},{"cell_type":"code","source":"model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\ndataset_name = \"keivalya/MedQuad-MedicalQnADataset\"","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:19.096609Z","iopub.execute_input":"2024-09-18T16:29:19.096912Z","iopub.status.idle":"2024-09-18T16:29:19.100967Z","shell.execute_reply.started":"2024-09-18T16:29:19.096880Z","shell.execute_reply":"2024-09-18T16:29:19.100057Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#We will use the transformers library to load the pre-trained LLaMA 3 model and its tokenizer.\n#This is a critical step to ensure that the model understands the format of the input data\n\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# # Specify the model ID from Hugging Face Model Hub\n# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n# tokenizer = AutoTokenizer.from_pretrained(model_id)\n# model = AutoModelForCausalLM.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n)\nmodel = AutoModelForCausalLM.from_pretrained( model_name, quantization_config=quant_config,torch_dtype=\"float16\", device_map=\"auto\")\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:29:19.102323Z","iopub.execute_input":"2024-09-18T16:29:19.102697Z","iopub.status.idle":"2024-09-18T16:31:21.959024Z","shell.execute_reply.started":"2024-09-18T16:29:19.102653Z","shell.execute_reply":"2024-09-18T16:31:21.958051Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a42f177ba8c4d40ae0de1de2df0bfdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d03c4483915c451e9e8fa24a1ebd90d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8c8711debaa43069de1f48a42b95e7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f30e36cc2884335b590c813aa6caea6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fbaa7019f3f45918430a51e307d5b51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"095619ba37f548d6acb8cf2cf4476eef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbdab15fe3be4b899b8bdeeb4d84f4c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"556dc6c1130946fe8dfbaaa8b8d44e76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77967a5c316647f8b5feb70b5f52655f"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Configure Tokenizer for Special Tokens:**\n\nSpecial tokens may be necessary for specific tasks, such as separating dialog turns or indicating the start of a response:","metadata":{}},{"cell_type":"code","source":"# Add or modify special tokens\nspecial_tokens_dict = {'additional_special_tokens': ['[USR]', '[SYS]']}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))  # Important: resize model token embeddings","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:21.960341Z","iopub.execute_input":"2024-09-18T16:31:21.960841Z","iopub.status.idle":"2024-09-18T16:31:22.053110Z","shell.execute_reply.started":"2024-09-18T16:31:21.960806Z","shell.execute_reply":"2024-09-18T16:31:22.052157Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Embedding(128258, 4096)"},"metadata":{}}]},{"cell_type":"markdown","source":"**Check Model and Tokenizer Compatibility:**\n\nEnsure that the model and tokenizer are aligned in terms of vocabulary size and token types, and test them with a simple encoding and decoding task:","metadata":{}},{"cell_type":"code","source":"# Test encoding and decoding to ensure alignment\nsample_text = \"Hello, how can I assist you today?\"\nencoded_input = tokenizer(sample_text, return_tensors='pt')\ndecoded_output = tokenizer.decode(encoded_input['input_ids'][0])\n\nprint(\"Encoded:\", encoded_input)\nprint(\"Decoded:\", decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:22.054340Z","iopub.execute_input":"2024-09-18T16:31:22.054655Z","iopub.status.idle":"2024-09-18T16:31:22.067529Z","shell.execute_reply.started":"2024-09-18T16:31:22.054623Z","shell.execute_reply":"2024-09-18T16:31:22.066556Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Encoded: {'input_ids': tensor([[128000,   9906,     11,   1268,    649,    358,   7945,    499,   3432,\n             30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\nDecoded: <|begin_of_text|>Hello, how can I assist you today?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ORPO Configuration and Training","metadata":{}},{"cell_type":"code","source":"#Define ORPO \n#The ORPO configuration involves setting parameters that control the training process, \n#including learning rates, batch sizes, and the preference alignment ratio\nfrom trl import ORPOConfig, ORPOTrainer\n\norpo_config = ORPOConfig(\n    output_dir='./output',  # Specify the output directory for saving models and checkpoints\n    learning_rate=8e-6,\n    beta=0.1,  # The lambda parameter for preference optimization\n    max_length=512,  # Maximum sequence length\n    per_device_train_batch_size=2,\n    num_train_epochs=1,  # For demonstration, a small number of epochs\n    logging_dir='./logs',  # Directory for storing logs\n    logging_steps=10\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:22.068718Z","iopub.execute_input":"2024-09-18T16:31:22.069075Z","iopub.status.idle":"2024-09-18T16:31:35.265130Z","shell.execute_reply.started":"2024-09-18T16:31:22.069034Z","shell.execute_reply":"2024-09-18T16:31:35.264322Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(train_dataset[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:35.266362Z","iopub.execute_input":"2024-09-18T16:31:35.266998Z","iopub.status.idle":"2024-09-18T16:31:35.277666Z","shell.execute_reply.started":"2024-09-18T16:31:35.266961Z","shell.execute_reply":"2024-09-18T16:31:35.276645Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"{'qtype': 'treatment', 'Question': 'What are the treatments for Lennox-Gastaut syndrome ?', 'Answer': 'These resources address the diagnosis or management of Lennox-Gastaut syndrome:  - Cleveland Clinic  - Genetic Testing Registry: Epileptic encephalopathy Lennox-Gastaut type  - National Institute of Neurological Disorders and Stroke: Diagnosis and Treatment of Epilepsy  - News Release: FDA Approves New Drug to Treat Severe Form of Epilepsy (U.S. Food and Drug Administration, November 20, 2008)   These resources from MedlinePlus offer information about the diagnosis and management of various health conditions:  - Diagnostic Tests  - Drug Therapy  - Surgery and Rehabilitation  - Genetic Counseling   - Palliative Care'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize ORPO Trainer:\n\n# The ORPO trainer handles the fine-tuning process by applying the ORPO algorithm to update the model’s weights based on the dataset\n# and the defined configuration:","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:35.279489Z","iopub.execute_input":"2024-09-18T16:31:35.279862Z","iopub.status.idle":"2024-09-18T16:31:35.325025Z","shell.execute_reply.started":"2024-09-18T16:31:35.279816Z","shell.execute_reply":"2024-09-18T16:31:35.324076Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# trainer = ORPOTrainer(\n#     model=model,\n#     args=orpo_config,\n#     train_dataset=train_dataset,\n#     eval_dataset=test_dataset,\n#     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:35.326370Z","iopub.execute_input":"2024-09-18T16:31:35.327291Z","iopub.status.idle":"2024-09-18T16:31:35.335060Z","shell.execute_reply.started":"2024-09-18T16:31:35.327245Z","shell.execute_reply":"2024-09-18T16:31:35.334181Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"llama_prompt = \"\"\"You are a medical assistance. Your task is to answer medical related queries.\n\n### Query:\n{}\n\n### Answer:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:35.336166Z","iopub.execute_input":"2024-09-18T16:31:35.336953Z","iopub.status.idle":"2024-09-18T16:31:35.344138Z","shell.execute_reply.started":"2024-09-18T16:31:35.336919Z","shell.execute_reply":"2024-09-18T16:31:35.343330Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n    questions = examples[\"Question\"]\n    answers = examples[\"Answer\"]\n    texts = []\n    for question, answer in zip(questions, answers):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = llama_prompt.format(question, answer) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\n\ntrain_dataset = train_dataset.map(formatting_prompts_func, batched = True)\ntest_dataset = test_dataset.map(formatting_prompts_func, batched = True)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:35.345138Z","iopub.execute_input":"2024-09-18T16:31:35.345448Z","iopub.status.idle":"2024-09-18T16:31:36.178134Z","shell.execute_reply.started":"2024-09-18T16:31:35.345416Z","shell.execute_reply":"2024-09-18T16:31:36.177202Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14766 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d86e580e8ec04feba370859d95832f32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1641 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3207f97f0a74ca7992e29d0fb49a04d"}},"metadata":{}}]},{"cell_type":"code","source":"# Initialize a WandB run\nwandb.init(project='llama3-finetuning', entity='rafayahmad-addo-ai')","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:36.179511Z","iopub.execute_input":"2024-09-18T16:31:36.179819Z","iopub.status.idle":"2024-09-18T16:31:37.924223Z","shell.execute_reply.started":"2024-09-18T16:31:36.179786Z","shell.execute_reply":"2024-09-18T16:31:37.923350Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240918_163136-dypbv78m</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning/runs/dypbv78m' target=\"_blank\">prime-glitter-14</a></strong> to <a href='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning' target=\"_blank\">https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning/runs/dypbv78m' target=\"_blank\">https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning/runs/dypbv78m</a>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rafayahmad-addo-ai/llama3-finetuning/runs/dypbv78m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7e2ff839a140>"},"metadata":{}}]},{"cell_type":"markdown","source":"## PEFT Fine tuning Configuration","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer,SFTConfig\noutput_dir=\"llama-3.1-fine-tuned-model\"\n\npeft_config = LoraConfig(\n    r=32,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['down_proj', 'gate_proj', 'o_proj', 'v_proj', 'up_proj', 'q_proj', 'k_proj'],\n)\n\ntraining_arguments = SFTConfig(\n    output_dir=output_dir,                    # directory to save and repository id\n    num_train_epochs=1,                       # number of training epochs\n    per_device_train_batch_size=2,            # batch size per device during training\n    gradient_accumulation_steps=4,            # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n    optim=\"adamw_8bit\",\n    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper                        \n    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n    weight_decay=0.01,\n    fp16=True,\n    bf16=False,\n    logging_steps = 1,\n    max_steps=50,\n    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n    report_to=\"wandb\",                  # report metrics to w&b\n)\n\n\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=1024,\n    packing=False,\n    dataset_kwargs={\n    \"add_special_tokens\": False,\n    \"append_concat_token\": False,\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:37.925399Z","iopub.execute_input":"2024-09-18T16:31:37.925691Z","iopub.status.idle":"2024-09-18T16:31:47.331613Z","shell.execute_reply.started":"2024-09-18T16:31:37.925658Z","shell.execute_reply":"2024-09-18T16:31:47.330533Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, dataset_kwargs. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14766 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d383c30fb5e49ad9ff0e9bc77785163"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1641 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1117c08449eb4da59c63cf97f5b62ffc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.pad_token_id = tokenizer.eos_token_id\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:47.332831Z","iopub.execute_input":"2024-09-18T16:31:47.333207Z","iopub.status.idle":"2024-09-18T16:31:47.338668Z","shell.execute_reply.started":"2024-09-18T16:31:47.333163Z","shell.execute_reply":"2024-09-18T16:31:47.337828Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:31:47.340040Z","iopub.execute_input":"2024-09-18T16:31:47.340422Z","iopub.status.idle":"2024-09-18T16:54:40.832690Z","shell.execute_reply.started":"2024-09-18T16:31:47.340380Z","shell.execute_reply":"2024-09-18T16:54:40.831804Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 22:22, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.588200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.895800</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.647800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.822800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.564900</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.272000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.339800</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.503000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.227900</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.332300</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.137300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.274700</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.298900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.559000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.128100</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.323200</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.081800</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.089900</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.154100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.800800</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.084600</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.066800</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.924500</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.964900</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.942100</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.091100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.919900</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.980200</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.692000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.957700</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.188000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.847700</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.128600</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.286600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.055700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.817600</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.100600</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.955900</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.003700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.240100</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.715600</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.995500</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.215300</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.405400</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.874900</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>1.069300</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.987400</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.980500</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.981400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.198400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=50, training_loss=1.1542865872383117, metrics={'train_runtime': 1371.8686, 'train_samples_per_second': 0.292, 'train_steps_per_second': 0.036, 'total_flos': 7367680759971840.0, 'train_loss': 1.1542865872383117, 'epoch': 0.027089259108763374})"},"metadata":{}}]},{"cell_type":"code","source":"#stop reporting to wandb\nwandb.finish()\n# save model\ntrainer.save_model(\"model\")\ntokenizer.save_pretrained(\"model\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### evaluate the model","metadata":{}},{"cell_type":"code","source":"from transformers import EvalPrediction\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(p: EvalPrediction):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\n        'accuracy': accuracy_score(p.label_ids, preds),\n        'f1': f1_score(p.label_ids, preds, average='weighted')\n    }","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:59:36.805360Z","iopub.execute_input":"2024-09-18T16:59:36.805746Z","iopub.status.idle":"2024-09-18T16:59:36.812295Z","shell.execute_reply.started":"2024-09-18T16:59:36.805710Z","shell.execute_reply":"2024-09-18T16:59:36.811509Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(\"Evaluation results:\", eval_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize the results","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Example: Plotting accuracy\nplt.figure(figsize=(10, 5))\nsns.lineplot(data=evaluation_results['eval_accuracy'], label='Test Accuracy')\nplt.title('Model Accuracy over Epochs')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyze and Interpret results\nsample_outputs = tokenizer.batch_decode(trainer.predict(test_dataset.sample(5)).predictions, skip_special_tokens=True)\nfor output in sample_outputs:\n    print(\"Model Output:\", output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pushing model into Hugging Face Hub","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\n# Login to Hugging Face\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n# Push to hub\ntrainer.push_to_hub(\"YourFineTunedModel\", use_auth_token=True)\ntokenizer.push_to_hub(\"YourFineTunedModel\", use_auth_token=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Integrate the Model into an Application:\n\nProvide guidance on how to integrate the model into a live environment or application. This might include API usage examples or embedding the model within a web service:","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the model from the hub\nmodel_pipeline = pipeline(\"text-generation\", model=\"YourUserName/YourFineTunedModel\")\n\n# Example usage\nuser_input = \"I feel stressed and overwhelmed.\"\nresponse = model_pipeline(user_input)[0]['generated_text']\nprint(\"Model Response:\", response)","metadata":{},"execution_count":null,"outputs":[]}]}